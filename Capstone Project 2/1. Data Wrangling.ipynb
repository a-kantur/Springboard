{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "active19 = pd.read_csv(\"active19.csv\")\n",
    "active20 = pd.read_csv(\"active20.csv\")\n",
    "lazy19 = pd.read_csv(\"lazy19.csv\")\n",
    "lazy20 = pd.read_csv(\"lazy20.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a text is obtained, we start with text normalization. Text normalization includes:\n",
    "- converting all letters to lower or upper case\n",
    "- converting numbers into words or removing numbers\n",
    "- removing punctuations, accent marks and other diacritics\n",
    "- removing white spaces\n",
    "- expanding abbreviations\n",
    "- removing stop words, sparse terms, and particular words\n",
    "- text canonicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "active19['clean_text']=active19.Text.apply(lambda x: x.lower())\n",
    "active20['clean_text']=active20.Text.apply(lambda x: x.lower())\n",
    "lazy19['clean_text']=lazy19.Text.apply(lambda x: x.lower())\n",
    "lazy20['clean_text']=lazy20.Text.apply(lambda x: x.lower())\n",
    "\n",
    "# Remove numbers\n",
    "active19['clean_text']=active19['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "active20['clean_text']=active20['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "lazy19['clean_text']=lazy19['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "lazy20['clean_text']=lazy20['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "# Remove punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "active19['clean_text']=active19['clean_text'].apply(lambda x: x.translate(translator))\n",
    "active20['clean_text']=active20['clean_text'].apply(lambda x: x.translate(translator))\n",
    "lazy19['clean_text']=lazy19['clean_text'].apply(lambda x: x.translate(translator))\n",
    "lazy20['clean_text']=lazy20['clean_text'].apply(lambda x: x.translate(translator))\n",
    "\n",
    "# Remove whitespaces\n",
    "active19['clean_text']=active19['clean_text'].apply(lambda x: x.strip())\n",
    "active20['clean_text']=active20['clean_text'].apply(lambda x: x.strip())\n",
    "lazy19['clean_text']=lazy19['clean_text'].apply(lambda x: x.strip())\n",
    "lazy20['clean_text']=lazy20['clean_text'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of english stopwords 186\n"
     ]
    }
   ],
   "source": [
    "# set 'SW' as english stopwords from NLTK and count them\n",
    "SW = set(stopwords.words('english'))\n",
    "# remove 'not' from set SW\n",
    "SW.remove('not')\n",
    "SW.add('amp') # add 'amp' that shows up in tweets after '&' as in '&amp' from set SW\n",
    "SW.add('¬ª') #left after punctuation removal\n",
    "SW.add('‚Äù') #left after punctuation removal\n",
    "SW.add('‚Äú') #left after punctuation removal\n",
    "SW.add('‚Äô ') #left after punctuation removal\n",
    "SW.add('im')\n",
    "SW.add(\"I'm\")\n",
    "SW.add('gon na')\n",
    "print('Number of english stopwords', len(SW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'about', 'whom', 'of', 'each', 'he', 'these', 'haven', 'and', 'as', 'yourself', 'him', 'me', 'yourselves', \"mightn't\", 'is', 'why', 'we', \"won't\", 'your', 'hadn', 'theirs', 'has', 'didn', 'don', 'but', \"hadn't\", \"weren't\", '¬ª', 'some', 'off', 'they', 'ours', 'had', 'with', 'before', 'further', 'all', '‚Äú', 'do', 'only', 'through', \"shan't\", 'their', 'wasn', 'where', 'his', 'same', 'its', 'himself', 'too', 'yours', 'were', 'very', 'themselves', 'being', 'such', 'now', \"it's\", 'any', 'hasn', 'to', 'nor', 'd', \"aren't\", 'gon na', 'if', 'most', 'again', 'our', 'from', 'which', \"wasn't\", 'then', 'those', 'after', \"mustn't\", 'can', 'isn', 'should', 'she', 'by', 'ourselves', 'weren', 'so', 'into', \"that'll\", 'her', 'when', 'did', 'doesn', '‚Äô ', \"don't\", 'an', 're', 'mustn', 'that', '‚Äù', 'are', 'it', 'against', 'amp', 's', 'was', 'what', 'in', 'under', 'here', 'at', 'above', 'during', 'shan', 'down', \"doesn't\", 'once', 'my', 'wouldn', 'both', 'than', 've', 'hers', 'am', 'm', 'just', 't', 'between', \"you'd\", 'll', \"she's\", 'i', \"wouldn't\", 'won', 'does', \"isn't\", 'because', 'the', \"didn't\", 'more', \"you'll\", 'few', 'up', \"needn't\", \"shouldn't\", 'a', 'on', \"should've\", 'myself', 'y', 'or', 'below', \"couldn't\", 'have', 'ma', 'will', 'who', 'been', 'own', 'other', 'o', 'them', \"I'm\", \"hasn't\", \"you've\", 'while', 'no', 'im', 'you', 'needn', \"you're\", 'herself', 'this', 'until', 'ain', 'there', 'how', 'having', 'itself', 'aren', 'be', \"haven't\", 'for', 'mightn', 'out', 'shouldn', 'over', 'couldn', 'doing'}\n"
     ]
    }
   ],
   "source": [
    "# view english stopwords\n",
    "print(SW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(x):\n",
    "    words = word_tokenize(x) # make a list of words\n",
    "    useful_words = [w for w in words if w not in SW]    # remove stopwords\n",
    "    return (' '.join(useful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords:\n",
    "active19['clean_text'] = active19['clean_text'].apply(lambda x: remove_stop_words(x))\n",
    "active20['clean_text'] = active20['clean_text'].apply(lambda x: remove_stop_words(x))\n",
    "lazy19['clean_text'] = lazy19['clean_text'].apply(lambda x: remove_stop_words(x))\n",
    "lazy20['clean_text'] = lazy20['clean_text'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anna_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# lemmatize\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "#lemmatizer.lemmatize(word)\n",
    "\n",
    "active19['clean_text'] = active19['clean_text'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "active20['clean_text'] = active20['clean_text'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "lazy19['clean_text'] = lazy19['clean_text'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "lazy20['clean_text'] = lazy20['clean_text'].apply(lambda x: lemmatizer.lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert emoji to text:\n",
    "active19['clean_text'] = active19['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))\n",
    "active20['clean_text'] = active20['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))\n",
    "lazy19['clean_text'] = lazy19['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))\n",
    "lazy20['clean_text'] = lazy20['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove ' and gon na:\n",
    "active19['clean_text'] = active19['clean_text'].str.replace(r'gon na', 'going')\n",
    "active20['clean_text'] = active20['clean_text'].str.replace(r'gon na', 'going')\n",
    "lazy19['clean_text'] = lazy19['clean_text'].str.replace(r'gon na', 'going')\n",
    "lazy20['clean_text'] = lazy20['clean_text'].str.replace(r'gon na', 'going')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>üòÇ Hundred percent intentional. Better to ask f...</td>\n",
       "      <td>2020-04-08</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>face_with_tears_of_joy hundred percent intenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I know I‚Äôm gonna see it! I‚Äôm turning my Twitte...</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>know ‚Äô going see ‚Äô turning twitter page alanya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Why am I crying? Oh, it‚Äôs just my friend John ...</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>crying oh ‚Äô friend john literally making world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>That‚Äôs gonna open a wormhole GTFOOT!!! https:/...</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>‚Äô going open wormhole gtfoot httpstcokufpiwt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Sure he was a writer and producer on ‚ÄúParks an...</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sure writer producer parks rec yes ‚Äô peabody e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text        Date  \\\n",
       "0  üòÇ Hundred percent intentional. Better to ask f...  2020-04-08   \n",
       "1  I know I‚Äôm gonna see it! I‚Äôm turning my Twitte...  2020-04-07   \n",
       "2  Why am I crying? Oh, it‚Äôs just my friend John ...  2020-04-06   \n",
       "3  That‚Äôs gonna open a wormhole GTFOOT!!! https:/...  2020-03-27   \n",
       "4  Sure he was a writer and producer on ‚ÄúParks an...  2020-03-27   \n",
       "\n",
       "              Name Location                                         clean_text  \n",
       "0  prattprattpratt      NaN  face_with_tears_of_joy hundred percent intenti...  \n",
       "1  prattprattpratt      NaN  know ‚Äô going see ‚Äô turning twitter page alanya...  \n",
       "2  prattprattpratt      NaN  crying oh ‚Äô friend john literally making world...  \n",
       "3  prattprattpratt      NaN       ‚Äô going open wormhole gtfoot httpstcokufpiwt  \n",
       "4  prattprattpratt      NaN  sure writer producer parks rec yes ‚Äô peabody e...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active20.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframes as csv for future analysis\n",
    "active20.reset_index().to_csv(\"active20-clean.csv\")\n",
    "active19.reset_index().to_csv(\"active19-clean.csv\")\n",
    "lazy20.reset_index().to_csv(\"lazy20-clean.csv\")\n",
    "lazy19.reset_index().to_csv(\"lazy19-clean.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
