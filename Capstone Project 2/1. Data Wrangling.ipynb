{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "active19 = pd.read_csv(\"active19.csv\")\n",
    "active20 = pd.read_csv(\"active20.csv\")\n",
    "lazy19 = pd.read_csv(\"lazy19.csv\")\n",
    "lazy20 = pd.read_csv(\"lazy20.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a text is obtained, we start with text normalization. Text normalization includes:\n",
    "- converting all letters to lower or upper case\n",
    "- converting numbers into words or removing numbers\n",
    "- removing punctuations, accent marks and other diacritics\n",
    "- removing white spaces\n",
    "- expanding abbreviations\n",
    "- removing stop words, sparse terms, and particular words\n",
    "- text canonicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "active19['clean_text']=active19.Text.apply(lambda x: x.lower())\n",
    "active20['clean_text']=active20.Text.apply(lambda x: x.lower())\n",
    "lazy19['clean_text']=lazy19.Text.apply(lambda x: x.lower())\n",
    "lazy20['clean_text']=lazy20.Text.apply(lambda x: x.lower())\n",
    "\n",
    "# Remove numbers\n",
    "active19['clean_text']=active19['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "active20['clean_text']=active20['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "lazy19['clean_text']=lazy19['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "lazy20['clean_text']=lazy20['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "# Remove punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "active19['clean_text']=active19['clean_text'].apply(lambda x: x.translate(translator))\n",
    "active20['clean_text']=active20['clean_text'].apply(lambda x: x.translate(translator))\n",
    "lazy19['clean_text']=lazy19['clean_text'].apply(lambda x: x.translate(translator))\n",
    "lazy20['clean_text']=lazy20['clean_text'].apply(lambda x: x.translate(translator))\n",
    "\n",
    "# Remove whitespaces\n",
    "active19['clean_text']=active19['clean_text'].apply(lambda x: x.strip())\n",
    "active20['clean_text']=active20['clean_text'].apply(lambda x: x.strip())\n",
    "lazy19['clean_text']=lazy19['clean_text'].apply(lambda x: x.strip())\n",
    "lazy20['clean_text']=lazy20['clean_text'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of english stopwords 179\n"
     ]
    }
   ],
   "source": [
    "# set 'SW' as english stopwords from NLTK and count them\n",
    "SW = set(stopwords.words('english'))\n",
    "# remove 'not' from set SW\n",
    "SW.remove('not')\n",
    "# add 'amp' that shows up in tweets after '&' as in '&amp' from set SW\n",
    "SW.add('amp')\n",
    "print('Number of english stopwords', len(SW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'were', 'through', 'such', 'to', 'that', 'my', 'has', 'from', 'was', 're', 'didn', 'on', 'have', 'with', 'why', 'ours', 'ourselves', 'how', 'and', 'any', 'up', 'more', 'this', \"needn't\", 'she', 'a', 'doesn', 'aren', 'once', 'had', 'doing', 'myself', 'yours', 'hers', 'shan', 'yourself', 'each', 'being', 'their', 'until', 'of', 'out', 'd', 'them', 'mustn', 'couldn', 'wouldn', 'needn', 'are', 'will', 'amp', 'into', \"she's\", 'those', 'over', 'll', 'after', 'wasn', 'herself', 'i', 'down', \"aren't\", 'weren', 'whom', 'where', 'or', 'other', 'no', \"don't\", \"it's\", \"wasn't\", \"couldn't\", 'then', 'above', \"you'd\", 'should', 'itself', 'having', 'we', 'he', 'him', 'your', 'does', 'shouldn', 'while', 'some', 'too', \"should've\", \"weren't\", 'nor', 'the', 'both', 'own', 'haven', 'than', \"isn't\", \"won't\", \"wouldn't\", \"you'll\", 'been', 'who', \"shan't\", 'won', 'be', 'am', 'between', 'for', 'its', 'only', 'y', 'same', \"haven't\", 'below', 'do', 'further', 'off', 'her', 'at', 'but', 'you', 'don', 'yourselves', 'few', 'these', 'isn', 'theirs', 'ain', 'o', 'hadn', \"hadn't\", 'here', 'there', 've', 'mightn', 'themselves', 'very', 'now', 'can', 'in', \"you're\", 'is', 'did', 'so', 'again', 'when', 'because', 'by', 'during', \"mustn't\", 'as', \"mightn't\", 'our', 'under', 'ma', 'me', 't', 'hasn', 'it', 'they', 'which', 'all', 'just', 'against', 'about', 's', 'most', \"shouldn't\", 'an', 'himself', 'his', \"didn't\", 'if', 'before', \"you've\", 'what', \"doesn't\", \"hasn't\", \"that'll\", 'm'}\n"
     ]
    }
   ],
   "source": [
    "# view english stopwords\n",
    "print(SW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(x):\n",
    "    words = word_tokenize(x) # make a list of words\n",
    "    useful_words = [w for w in words if w not in SW]    # remove stopwords\n",
    "    return (' '.join(useful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords:\n",
    "active19['clean_text'] = active19['clean_text'].apply(lambda x: remove_stop_words(x))\n",
    "active20['clean_text'] = active20['clean_text'].apply(lambda x: remove_stop_words(x))\n",
    "lazy19['clean_text'] = lazy19['clean_text'].apply(lambda x: remove_stop_words(x))\n",
    "lazy20['clean_text'] = lazy20['clean_text'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert emoji to text:\n",
    "active19['clean_text'] = active19['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))\n",
    "active20['clean_text'] = active20['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))\n",
    "lazy19['clean_text'] = lazy19['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))\n",
    "lazy20['clean_text'] = lazy20['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave only alpha:\n",
    "#active19['clean_text'] = active19['clean_text'].apply(lambda x: re.sub(r'[^A-Za-z]+',' ',x))\n",
    "#active20['clean_text'] = active20['clean_text'].apply(lambda x: re.sub(r'[^A-Za-z]+',' ',x))\n",
    "#lazy19['clean_text'] = lazy19['clean_text'].apply(lambda x: re.sub(r'[^A-Za-z]+',' ',x))\n",
    "#lazy20['clean_text'] = lazy20['clean_text'].apply(lambda x: re.sub(r'[^A-Za-z]+',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>😂 Hundred percent intentional. Better to ask f...</td>\n",
       "      <td>2020-04-08</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>face_with_tears_of_joy hundred percent intenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I know I’m gonna see it! I’m turning my Twitte...</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>know ’ gon na see ’ turning twitter page alany...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Why am I crying? Oh, it’s just my friend John ...</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>crying oh ’ friend john literally making world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>That’s gonna open a wormhole GTFOOT!!! https:/...</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>’ gon na open wormhole gtfoot httpstcokufpiwt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Sure he was a writer and producer on “Parks an...</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sure writer producer “ parks rec ” yes ’ peabo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Exciting news✨ #PixarOnward is ON DIGITAL NOW ...</td>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exciting newssparkles pixaronward digital us a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>I came downstairs, saw Katherine crying her ey...</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>came downstairs saw katherine crying eyes thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Gary Cooper in “The Westerner” https://t.co/mk...</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gary cooper “ westerner ” httpstcomkjifqxmeq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>I liked that https://t.co/lXGtDzIciV</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>liked httpstcolxgtdziciv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>I’m so proud of my darling on the success of h...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>’ proud darling success book smartly delayed r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text        Date  \\\n",
       "0  😂 Hundred percent intentional. Better to ask f...  2020-04-08   \n",
       "1  I know I’m gonna see it! I’m turning my Twitte...  2020-04-07   \n",
       "2  Why am I crying? Oh, it’s just my friend John ...  2020-04-06   \n",
       "3  That’s gonna open a wormhole GTFOOT!!! https:/...  2020-03-27   \n",
       "4  Sure he was a writer and producer on “Parks an...  2020-03-27   \n",
       "5  Exciting news✨ #PixarOnward is ON DIGITAL NOW ...  2020-03-20   \n",
       "6  I came downstairs, saw Katherine crying her ey...  2020-03-18   \n",
       "7  Gary Cooper in “The Westerner” https://t.co/mk...  2020-03-16   \n",
       "8               I liked that https://t.co/lXGtDzIciV  2020-03-14   \n",
       "9  I’m so proud of my darling on the success of h...  2020-03-13   \n",
       "\n",
       "              Name Location                                         clean_text  \n",
       "0  prattprattpratt      NaN  face_with_tears_of_joy hundred percent intenti...  \n",
       "1  prattprattpratt      NaN  know ’ gon na see ’ turning twitter page alany...  \n",
       "2  prattprattpratt      NaN  crying oh ’ friend john literally making world...  \n",
       "3  prattprattpratt      NaN      ’ gon na open wormhole gtfoot httpstcokufpiwt  \n",
       "4  prattprattpratt      NaN  sure writer producer “ parks rec ” yes ’ peabo...  \n",
       "5  prattprattpratt      NaN  exciting newssparkles pixaronward digital us a...  \n",
       "6  prattprattpratt      NaN  came downstairs saw katherine crying eyes thou...  \n",
       "7  prattprattpratt      NaN       gary cooper “ westerner ” httpstcomkjifqxmeq  \n",
       "8  prattprattpratt      NaN                           liked httpstcolxgtdziciv  \n",
       "9  prattprattpratt      NaN  ’ proud darling success book smartly delayed r...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active20[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframes as csv for future analysis\n",
    "active20.reset_index().to_csv(\"active20-clean.csv\")\n",
    "active19.reset_index().to_csv(\"active19-clean.csv\")\n",
    "lazy20.reset_index().to_csv(\"lazy20-clean.csv\")\n",
    "lazy19.reset_index().to_csv(\"lazy19-clean.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
