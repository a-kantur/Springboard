{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "active19 = pd.read_csv(\"active19.csv\")\n",
    "active20 = pd.read_csv(\"active20.csv\")\n",
    "lazy19 = pd.read_csv(\"lazy19.csv\")\n",
    "lazy20 = pd.read_csv(\"lazy20.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a text is obtained, we start with text normalization. Text normalization includes:\n",
    "- converting all letters to lower or upper case\n",
    "- converting numbers into words or removing numbers\n",
    "- removing punctuations, accent marks and other diacritics\n",
    "- removing white spaces\n",
    "- expanding abbreviations\n",
    "- removing stop words, sparse terms, and particular words\n",
    "- text canonicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "active19['clean_text']=active19.Text.apply(lambda x: x.lower())\n",
    "active20['clean_text']=active20.Text.apply(lambda x: x.lower())\n",
    "lazy19['clean_text']=lazy19.Text.apply(lambda x: x.lower())\n",
    "lazy20['clean_text']=lazy20.Text.apply(lambda x: x.lower())\n",
    "\n",
    "# Remove numbers\n",
    "active19['clean_text']=active19['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "active20['clean_text']=active20['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "lazy19['clean_text']=lazy19['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "lazy20['clean_text']=lazy20['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "# Remove punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "active19['clean_text']=active19['clean_text'].apply(lambda x: x.translate(translator))\n",
    "active20['clean_text']=active20['clean_text'].apply(lambda x: x.translate(translator))\n",
    "lazy19['clean_text']=lazy19['clean_text'].apply(lambda x: x.translate(translator))\n",
    "lazy20['clean_text']=lazy20['clean_text'].apply(lambda x: x.translate(translator))\n",
    "\n",
    "# Remove whitespaces\n",
    "active19['clean_text']=active19['clean_text'].apply(lambda x: x.strip())\n",
    "active20['clean_text']=active20['clean_text'].apply(lambda x: x.strip())\n",
    "lazy19['clean_text']=lazy19['clean_text'].apply(lambda x: x.strip())\n",
    "lazy20['clean_text']=lazy20['clean_text'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of english stopwords 179\n"
     ]
    }
   ],
   "source": [
    "# set 'SW' as english stopwords from NLTK and count them\n",
    "SW = set(stopwords.words('english'))\n",
    "# remove 'not' from set SW\n",
    "SW.remove('not')\n",
    "# add 'amp' that shows up in tweets after '&' as in '&amp' from set SW\n",
    "SW.add('amp')\n",
    "print('Number of english stopwords', len(SW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'were', 'through', 'such', 'to', 'that', 'my', 'has', 'from', 'was', 're', 'didn', 'on', 'have', 'with', 'why', 'ours', 'ourselves', 'how', 'and', 'any', 'up', 'more', 'this', \"needn't\", 'she', 'a', 'doesn', 'aren', 'once', 'had', 'doing', 'myself', 'yours', 'hers', 'shan', 'yourself', 'each', 'being', 'their', 'until', 'of', 'out', 'd', 'them', 'mustn', 'couldn', 'wouldn', 'needn', 'are', 'will', 'amp', 'into', \"she's\", 'those', 'over', 'll', 'after', 'wasn', 'herself', 'i', 'down', \"aren't\", 'weren', 'whom', 'where', 'or', 'other', 'no', \"don't\", \"it's\", \"wasn't\", \"couldn't\", 'then', 'above', \"you'd\", 'should', 'itself', 'having', 'we', 'he', 'him', 'your', 'does', 'shouldn', 'while', 'some', 'too', \"should've\", \"weren't\", 'nor', 'the', 'both', 'own', 'haven', 'than', \"isn't\", \"won't\", \"wouldn't\", \"you'll\", 'been', 'who', \"shan't\", 'won', 'be', 'am', 'between', 'for', 'its', 'only', 'y', 'same', \"haven't\", 'below', 'do', 'further', 'off', 'her', 'at', 'but', 'you', 'don', 'yourselves', 'few', 'these', 'isn', 'theirs', 'ain', 'o', 'hadn', \"hadn't\", 'here', 'there', 've', 'mightn', 'themselves', 'very', 'now', 'can', 'in', \"you're\", 'is', 'did', 'so', 'again', 'when', 'because', 'by', 'during', \"mustn't\", 'as', \"mightn't\", 'our', 'under', 'ma', 'me', 't', 'hasn', 'it', 'they', 'which', 'all', 'just', 'against', 'about', 's', 'most', \"shouldn't\", 'an', 'himself', 'his', \"didn't\", 'if', 'before', \"you've\", 'what', \"doesn't\", \"hasn't\", \"that'll\", 'm'}\n"
     ]
    }
   ],
   "source": [
    "# view english stopwords\n",
    "print(SW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(x):\n",
    "    words = word_tokenize(x) # make a list of words\n",
    "    useful_words = [w for w in words if w not in SW]    # remove stopwords\n",
    "    return (' '.join(useful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords:\n",
    "active19['clean_text'] = active19['clean_text'].apply(lambda x: remove_stop_words(x))\n",
    "active20['clean_text'] = active20['clean_text'].apply(lambda x: remove_stop_words(x))\n",
    "lazy19['clean_text'] = lazy19['clean_text'].apply(lambda x: remove_stop_words(x))\n",
    "lazy20['clean_text'] = lazy20['clean_text'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert emoji to text:\n",
    "active19['clean_text'] = active19['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))\n",
    "active20['clean_text'] = active20['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))\n",
    "lazy19['clean_text'] = lazy19['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))\n",
    "lazy20['clean_text'] = lazy20['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave only alpha:\n",
    "#active19['clean_text'] = active19['clean_text'].apply(lambda x: re.sub(r'[^A-Za-z]+',' ',x))\n",
    "#active20['clean_text'] = active20['clean_text'].apply(lambda x: re.sub(r'[^A-Za-z]+',' ',x))\n",
    "#lazy19['clean_text'] = lazy19['clean_text'].apply(lambda x: re.sub(r'[^A-Za-z]+',' ',x))\n",
    "#lazy20['clean_text'] = lazy20['clean_text'].apply(lambda x: re.sub(r'[^A-Za-z]+',' ',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>üòÇ Hundred percent intentional. Better to ask f...</td>\n",
       "      <td>2020-04-08</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>face_with_tears_of_joy hundred percent intenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I know I‚Äôm gonna see it! I‚Äôm turning my Twitte...</td>\n",
       "      <td>2020-04-07</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>know ‚Äô gon na see ‚Äô turning twitter page alany...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Why am I crying? Oh, it‚Äôs just my friend John ...</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>crying oh ‚Äô friend john literally making world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>That‚Äôs gonna open a wormhole GTFOOT!!! https:/...</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>‚Äô gon na open wormhole gtfoot httpstcokufpiwt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Sure he was a writer and producer on ‚ÄúParks an...</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sure writer producer ‚Äú parks rec ‚Äù yes ‚Äô peabo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Exciting news‚ú® #PixarOnward is ON DIGITAL NOW ...</td>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>exciting newssparkles pixaronward digital us a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>I came downstairs, saw Katherine crying her ey...</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>came downstairs saw katherine crying eyes thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Gary Cooper in ‚ÄúThe Westerner‚Äù https://t.co/mk...</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gary cooper ‚Äú westerner ‚Äù httpstcomkjifqxmeq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>I liked that https://t.co/lXGtDzIciV</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>liked httpstcolxgtdziciv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>I‚Äôm so proud of my darling on the success of h...</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>prattprattpratt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>‚Äô proud darling success book smartly delayed r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text        Date  \\\n",
       "0  üòÇ Hundred percent intentional. Better to ask f...  2020-04-08   \n",
       "1  I know I‚Äôm gonna see it! I‚Äôm turning my Twitte...  2020-04-07   \n",
       "2  Why am I crying? Oh, it‚Äôs just my friend John ...  2020-04-06   \n",
       "3  That‚Äôs gonna open a wormhole GTFOOT!!! https:/...  2020-03-27   \n",
       "4  Sure he was a writer and producer on ‚ÄúParks an...  2020-03-27   \n",
       "5  Exciting news‚ú® #PixarOnward is ON DIGITAL NOW ...  2020-03-20   \n",
       "6  I came downstairs, saw Katherine crying her ey...  2020-03-18   \n",
       "7  Gary Cooper in ‚ÄúThe Westerner‚Äù https://t.co/mk...  2020-03-16   \n",
       "8               I liked that https://t.co/lXGtDzIciV  2020-03-14   \n",
       "9  I‚Äôm so proud of my darling on the success of h...  2020-03-13   \n",
       "\n",
       "              Name Location                                         clean_text  \n",
       "0  prattprattpratt      NaN  face_with_tears_of_joy hundred percent intenti...  \n",
       "1  prattprattpratt      NaN  know ‚Äô gon na see ‚Äô turning twitter page alany...  \n",
       "2  prattprattpratt      NaN  crying oh ‚Äô friend john literally making world...  \n",
       "3  prattprattpratt      NaN      ‚Äô gon na open wormhole gtfoot httpstcokufpiwt  \n",
       "4  prattprattpratt      NaN  sure writer producer ‚Äú parks rec ‚Äù yes ‚Äô peabo...  \n",
       "5  prattprattpratt      NaN  exciting newssparkles pixaronward digital us a...  \n",
       "6  prattprattpratt      NaN  came downstairs saw katherine crying eyes thou...  \n",
       "7  prattprattpratt      NaN       gary cooper ‚Äú westerner ‚Äù httpstcomkjifqxmeq  \n",
       "8  prattprattpratt      NaN                           liked httpstcolxgtdziciv  \n",
       "9  prattprattpratt      NaN  ‚Äô proud darling success book smartly delayed r...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active20[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframes as csv for future analysis\n",
    "active20.reset_index().to_csv(\"active20-clean.csv\")\n",
    "active19.reset_index().to_csv(\"active19-clean.csv\")\n",
    "lazy20.reset_index().to_csv(\"lazy20-clean.csv\")\n",
    "lazy19.reset_index().to_csv(\"lazy19-clean.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
