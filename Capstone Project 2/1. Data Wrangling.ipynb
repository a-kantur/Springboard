{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "active19 = pd.read_csv(\"active19.csv\")\n",
    "active20 = pd.read_csv(\"active20.csv\")\n",
    "lazy19 = pd.read_csv(\"lazy19.csv\")\n",
    "lazy20 = pd.read_csv(\"lazy20.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a text is obtained, we start with text normalization. Text normalization includes:\n",
    "- converting all letters to lower or upper case\n",
    "- converting numbers into words or removing numbers\n",
    "- removing punctuations, accent marks and other diacritics\n",
    "- removing white spaces\n",
    "- expanding abbreviations\n",
    "- removing stop words, sparse terms, and particular words\n",
    "- text canonicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "active19['clean_text']=active19.Text.apply(lambda x: x.lower())\n",
    "active20['clean_text']=active20.Text.apply(lambda x: x.lower())\n",
    "lazy19['clean_text']=lazy19.Text.apply(lambda x: x.lower())\n",
    "lazy20['clean_text']=lazy20.Text.apply(lambda x: x.lower())\n",
    "\n",
    "# Remove numbers\n",
    "active19['clean_text']=active19['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "active20['clean_text']=active20['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "lazy19['clean_text']=lazy19['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "lazy20['clean_text']=lazy20['clean_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "# Remove punctuation\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "active19['clean_text']=active19['clean_text'].apply(lambda x: x.translate(translator))\n",
    "active20['clean_text']=active20['clean_text'].apply(lambda x: x.translate(translator))\n",
    "lazy19['clean_text']=lazy19['clean_text'].apply(lambda x: x.translate(translator))\n",
    "lazy20['clean_text']=lazy20['clean_text'].apply(lambda x: x.translate(translator))\n",
    "\n",
    "# Remove whitespaces\n",
    "active19['clean_text']=active19['clean_text'].apply(lambda x: x.strip())\n",
    "active20['clean_text']=active20['clean_text'].apply(lambda x: x.strip())\n",
    "lazy19['clean_text']=lazy19['clean_text'].apply(lambda x: x.strip())\n",
    "lazy20['clean_text']=lazy20['clean_text'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of english stopwords 186\n"
     ]
    }
   ],
   "source": [
    "# set 'SW' as english stopwords from NLTK and count them\n",
    "SW = set(stopwords.words('english'))\n",
    "# remove 'not' from set SW\n",
    "SW.remove('not')\n",
    "SW.add('amp') # add 'amp' that shows up in tweets after '&' as in '&amp' from set SW\n",
    "SW.add('»') #left after punctuation removal\n",
    "SW.add('”') #left after punctuation removal\n",
    "SW.add('“') #left after punctuation removal\n",
    "SW.add('’ ') #left after punctuation removal\n",
    "SW.add('im')\n",
    "SW.add(\"I'm\")\n",
    "SW.add('gon na')\n",
    "print('Number of english stopwords', len(SW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'but', 'his', 'there', '»', 'to', 'that', 'or', \"you'd\", 'isn', 'out', 'than', 'very', \"won't\", 'her', 'most', 'll', 'are', 'did', 'over', \"wouldn't\", 'yourself', 'again', 'after', 'under', 'just', 'gon na', 'at', 'who', \"that'll\", \"you've\", 've', 'weren', 'more', 'too', 'those', 'and', 'which', \"doesn't\", 'haven', 'other', 'during', '’ ', 'she', 'what', \"she's\", \"it's\", \"mustn't\", 'do', 'we', 'against', \"you're\", 'him', 'above', 'themselves', 'up', 'both', 'been', 'they', \"shan't\", \"shouldn't\", 'amp', 'is', 'can', 'being', 'needn', 'off', 'm', 'wasn', \"hasn't\", 'mightn', 'myself', \"needn't\", 'y', 'ma', 'me', 'where', 'o', 'down', 'mustn', 'does', 'should', 'as', \"mightn't\", 'a', 'same', 'doesn', 'shan', '“', 'own', 'in', \"wasn't\", 'ours', 'with', 'won', \"I'm\", 'have', 'its', 'until', 't', 'for', \"don't\", 'each', 'was', 's', \"you'll\", 'you', 'into', 'it', 'before', 'few', 'has', 'don', \"isn't\", 'im', 'then', 'the', 'whom', 'through', 'were', 'so', 'further', \"couldn't\", 'because', 'all', 'this', 'if', 'some', 'am', 'be', 'will', 'now', \"weren't\", 'how', 'himself', 'why', 'herself', 'my', 'yourselves', \"should've\", 'd', 'below', 'only', 'our', \"hadn't\", \"haven't\", 'nor', \"didn't\", 'these', 'once', 'hasn', 'by', 'doing', 'having', 'an', 'from', 'no', 'aren', 'couldn', 'shouldn', 'wouldn', 'didn', \"aren't\", 'your', 'he', 'itself', 'between', 'here', 'ourselves', 'yours', 'hers', 'while', 'had', 'any', 'about', 'such', 'of', 'on', 'their', 'ain', 'i', 'theirs', 're', 'hadn', 'them', 'when', '”'}\n"
     ]
    }
   ],
   "source": [
    "# view english stopwords\n",
    "print(SW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(x):\n",
    "    words = word_tokenize(x) # make a list of words\n",
    "    useful_words = [w for w in words if w not in SW]    # remove stopwords\n",
    "    return (' '.join(useful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords:\n",
    "active19['clean_text'] = active19['clean_text'].apply(lambda x: remove_stop_words(x))\n",
    "active20['clean_text'] = active20['clean_text'].apply(lambda x: remove_stop_words(x))\n",
    "lazy19['clean_text'] = lazy19['clean_text'].apply(lambda x: remove_stop_words(x))\n",
    "lazy20['clean_text'] = lazy20['clean_text'].apply(lambda x: remove_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anna_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# lemmatize\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "#lemmatizer.lemmatize(word)\n",
    "\n",
    "active19['clean_text'] = active19['clean_text'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "active20['clean_text'] = active20['clean_text'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "lazy19['clean_text'] = lazy19['clean_text'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "lazy20['clean_text'] = lazy20['clean_text'].apply(lambda x: lemmatizer.lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert emoji to text:\n",
    "active19['clean_text'] = active19['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))\n",
    "active20['clean_text'] = active20['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))\n",
    "lazy19['clean_text'] = lazy19['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))\n",
    "lazy20['clean_text'] = lazy20['clean_text'].apply(lambda x: emoji.demojize(x, use_aliases=False, delimiters=('', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove ' and gon na:\n",
    "active19['clean_text'] = active19['clean_text'].str.replace(r'gon na', 'going')\n",
    "active20['clean_text'] = active20['clean_text'].str.replace(r'gon na', 'going')\n",
    "lazy19['clean_text'] = lazy19['clean_text'].str.replace(r'gon na', 'going')\n",
    "lazy20['clean_text'] = lazy20['clean_text'].str.replace(r'gon na', 'going')\n",
    "\n",
    "active19['clean_text'] = active19['clean_text'].str.replace(r'’ ', '')\n",
    "active20['clean_text'] = active20['clean_text'].str.replace(r'’ ', '')\n",
    "lazy19['clean_text'] = lazy19['clean_text'].str.replace(r'’ ', '')\n",
    "lazy20['clean_text'] = lazy20['clean_text'].str.replace(r'’ ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>search terms</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Look honors bio did too but tbh I didn’t reall...</td>\n",
       "      <td>2020-04-11</td>\n",
       "      <td>HappyFeminist</td>\n",
       "      <td>New Jersey, USA</td>\n",
       "      <td>homebody</td>\n",
       "      <td>look honors bio tbh really pay attention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>IT. IS. A. VIRUS. \\n\\nOur high school sex ed c...</td>\n",
       "      <td>2020-04-11</td>\n",
       "      <td>HappyFeminist</td>\n",
       "      <td>New Jersey, USA</td>\n",
       "      <td>homebody</td>\n",
       "      <td>virus high school sex ed class taught us diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>.@NickBollettieri webinar @MyUTR is absolutely...</td>\n",
       "      <td>2020-04-09</td>\n",
       "      <td>Riske4rewards</td>\n",
       "      <td>NaN</td>\n",
       "      <td>homebody</td>\n",
       "      <td>nickbollettieri webinar myutr absolutely price...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Join me tomorrow at 2pm PT for my live UTR All...</td>\n",
       "      <td>2020-04-06</td>\n",
       "      <td>Riske4rewards</td>\n",
       "      <td>NaN</td>\n",
       "      <td>homebody</td>\n",
       "      <td>join tomorrow pm pt live utr access series web...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>my current fav hobby is browsing https://t.co/...</td>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>Riske4rewards</td>\n",
       "      <td>NaN</td>\n",
       "      <td>homebody</td>\n",
       "      <td>current fav hobby browsing httpstcovewmjay hea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text        Date  \\\n",
       "0  Look honors bio did too but tbh I didn’t reall...  2020-04-11   \n",
       "1  IT. IS. A. VIRUS. \\n\\nOur high school sex ed c...  2020-04-11   \n",
       "2  .@NickBollettieri webinar @MyUTR is absolutely...  2020-04-09   \n",
       "3  Join me tomorrow at 2pm PT for my live UTR All...  2020-04-06   \n",
       "4  my current fav hobby is browsing https://t.co/...  2020-04-05   \n",
       "\n",
       "            Name         Location search terms  \\\n",
       "0  HappyFeminist  New Jersey, USA     homebody   \n",
       "1  HappyFeminist  New Jersey, USA     homebody   \n",
       "2  Riske4rewards              NaN     homebody   \n",
       "3  Riske4rewards              NaN     homebody   \n",
       "4  Riske4rewards              NaN     homebody   \n",
       "\n",
       "                                          clean_text  \n",
       "0           look honors bio tbh really pay attention  \n",
       "1  virus high school sex ed class taught us diffe...  \n",
       "2  nickbollettieri webinar myutr absolutely price...  \n",
       "3  join tomorrow pm pt live utr access series web...  \n",
       "4  current fav hobby browsing httpstcovewmjay hea...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy20.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataframes as csv for future analysis\n",
    "active20.reset_index().to_csv(\"active20-clean.csv\")\n",
    "active19.reset_index().to_csv(\"active19-clean.csv\")\n",
    "lazy20.reset_index().to_csv(\"lazy20-clean.csv\")\n",
    "lazy19.reset_index().to_csv(\"lazy19-clean.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
