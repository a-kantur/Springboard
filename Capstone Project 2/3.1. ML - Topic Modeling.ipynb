{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "active20=pd.read_csv(\"active20-clean.csv\")\n",
    "active19=pd.read_csv(\"active19-clean.csv\")\n",
    "lazy20=pd.read_csv(\"lazy20-clean.csv\")\n",
    "lazy19=pd.read_csv(\"lazy19-clean.csv\")\n",
    "twitter_users = pd.read_csv(\"Twitter users.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this section we will try to answer the question: <br>\n",
    "What Did Active and Lazy User Cohorts Discuss on Twitter in Spring 2019 and in Spring 2020?<br>\n",
    "To answer this question, we will use topic modeling. Topic modeling is an unsupervised machine learning technique. This means it can infer patterns and cluster similar expressions without needing to define topic tags or train data beforehand. We will use this approach to identify main topics of dicsussion in 2019 and 2020 in both cohorts. Based on the EDA findings, we expect to see weather-related topic in active 2019 tweets, game-related topics in lazy 2019 tweets and covid-related topic in both active and lazy in 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling algorithms are statistical methods that analyze the words of documents to discover the themes that pervade a large collection of documents. The basic idea of topic modeling is that a document is a mixture of latent topics and each topic is expressed by a distribution of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA) Model in gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) is the most popular topic modeling method in the field of text mining. The output of LDA provides two probability matrices: 1) the (posterior) probability distribution of each document over the topics, and 2) the probability distribution of words in a given topic. The calculated probability matrixes are used to make inference about the topics and documents for text mining. LDA has been shown to be an effective tool for text mining of large datasets. We will then do topic model-derived clustering based on highest probable topic assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of documents from the preprocessed tweets:\n",
    "active19_docs=active19.clean_text.dropna()\n",
    "active19_docs=active19_docs.to_list()\n",
    "active20_docs=active20.clean_text.dropna()\n",
    "active20_docs=active20_docs.to_list()\n",
    "lazy19_docs=lazy19.clean_text.dropna()\n",
    "lazy19_docs=lazy19_docs.to_list()\n",
    "lazy20_docs=lazy20.clean_text.dropna()\n",
    "lazy20_docs=lazy20_docs.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# tokenize the tweet docs (we lemmatize the words in tweet docs \n",
    "# in order to reduce inflectional forms to a common base form):\n",
    "active19_docs_tokenized = [word_tokenize(doc) for doc in active19_docs]\n",
    "active20_docs_tokenized = [word_tokenize(doc) for doc in active20_docs]\n",
    "lazy19_docs_tokenized = [word_tokenize(doc) for doc in lazy19_docs]\n",
    "lazy20_docs_tokenized = [word_tokenize(doc) for doc in lazy20_docs]\n",
    "\n",
    "# make a gensim dictionary with tokenized tweet docs:\n",
    "dict_active19 = Dictionary(active19_docs_tokenized)\n",
    "dict_active20 = Dictionary(active20_docs_tokenized)\n",
    "dict_lazy19 = Dictionary(lazy19_docs_tokenized)\n",
    "dict_lazy20 = Dictionary(lazy20_docs_tokenized)\n",
    "\n",
    "# create a gensim corpus (different from the regular corpus in so that \n",
    "# each document is converted to a bag of words using token ids):\n",
    "active19_corpus = [dict_active19.doc2bow(doc) for doc in active19_docs_tokenized] \n",
    "active20_corpus = [dict_active20.doc2bow(doc) for doc in active20_docs_tokenized] \n",
    "lazy19_corpus = [dict_lazy19.doc2bow(doc) for doc in lazy19_docs_tokenized] \n",
    "lazy20_corpus = [dict_lazy20.doc2bow(doc) for doc in lazy20_docs_tokenized] \n",
    "\n",
    "# how to go to the word behind a token\n",
    "# dict_active19.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new', 'knife', 'post', 'forum', 'new', 'opinel', 'httpstcoprbmwht', 'knives']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active19_docs_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD-IDF model to define word frequency:\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf_active19 = models.TfidfModel(active19_corpus)\n",
    "corpus_tfidf_active19 = tfidf_active19[active19_corpus]\n",
    "#from pprint import pprint\n",
    "#for doc in corpus_tfidf_active19:\n",
    "#    pprint(doc)\n",
    "#    break\n",
    "tfidf_active20 = models.TfidfModel(active20_corpus)\n",
    "corpus_tfidf_active20 = tfidf_active20[active20_corpus]\n",
    "\n",
    "tfidf_lazy19 = models.TfidfModel(lazy19_corpus)\n",
    "corpus_tfidf_lazy19 = tfidf_lazy19[lazy19_corpus]\n",
    "\n",
    "tfidf_lazy20 = models.TfidfModel(lazy20_corpus)\n",
    "corpus_tfidf_lazy20 = tfidf_lazy20[lazy20_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Topics for Active Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running LDA using TF-IDF\n",
    "\n",
    "# Active 19\n",
    "lda_model_tfidf_active19 = gensim.models.LdaMulticore(corpus_tfidf_active19, num_topics=3, id2word=dict_active19, passes =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      " Word: 0.001*\"new\" + 0.001*\"facebook\" + 0.001*\"check\" + 0.001*\"see\" + 0.001*\"video\" + 0.001*\"great\" + 0.001*\"not\" + 0.001*\"posted\" + 0.001*\"get\" + 0.001*\"love\" + 0.001*\"one\" + 0.001*\"us\" + 0.001*\"day\" + 0.001*\"week\" + 0.001*\"today\" + 0.001*\"come\" + 0.001*\"spring\" + 0.001*\"ready\" + 0.001*\"weekend\" + 0.001*\"join\" + 0.001*\"morning\" + 0.001*\"go\" + 0.001*\"like\" + 0.001*\"work\" + 0.001*\"find\"\n",
      "\n",
      "Topic: 1 \n",
      " Word: 0.001*\"spring\" + 0.001*\"great\" + 0.001*\"new\" + 0.001*\"one\" + 0.001*\"us\" + 0.001*\"get\" + 0.001*\"love\" + 0.001*\"time\" + 0.001*\"check\" + 0.001*\"happy\" + 0.001*\"day\" + 0.001*\"please\" + 0.001*\"fun\" + 0.001*\"see\" + 0.001*\"video\" + 0.001*\"last\" + 0.001*\"going\" + 0.001*\"like\" + 0.001*\"life\" + 0.001*\"live\" + 0.001*\"back\" + 0.001*\"good\" + 0.001*\"night\" + 0.001*\"today\" + 0.001*\"dont\"\n",
      "\n",
      "Topic: 2 \n",
      " Word: 0.001*\"day\" + 0.001*\"new\" + 0.001*\"great\" + 0.001*\"time\" + 0.001*\"hiking\" + 0.001*\"check\" + 0.001*\"today\" + 0.001*\"april\" + 0.001*\"good\" + 0.001*\"video\" + 0.001*\"one\" + 0.001*\"best\" + 0.001*\"season\" + 0.001*\"get\" + 0.001*\"us\" + 0.001*\"th\" + 0.001*\"like\" + 0.001*\"go\" + 0.001*\"via\" + 0.001*\"camp\" + 0.001*\"facebook\" + 0.001*\"life\" + 0.001*\"adventure\" + 0.001*\"look\" + 0.001*\"looking\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf_active19.print_topics(num_topics=-1,num_words=25):\n",
    "    print('Topic: {} \\n Word: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from LDA model with term frequency, in 2019 the active users Twitter topics can be split into 3 groups:<br>\n",
    "1) Happy time hiking <br>\n",
    "2) Social media (Facebook, videos) about the weekend fun outdoors  <br> \n",
    "3) Social media (Facebook, videos) about camping and the Spring season <br>\n",
    "However, term frequency was almost the same, and none of the words in any topic were particularly important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active 20\n",
    "lda_model_tfidf_active20 = gensim.models.LdaMulticore(corpus_tfidf_active20, num_topics=3, id2word=dict_active20, passes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      " Word: 0.002*\"covid\" + 0.001*\"day\" + 0.001*\"us\" + 0.001*\"time\" + 0.001*\"coronavirus\" + 0.001*\"good\" + 0.001*\"not\" + 0.001*\"today\" + 0.001*\"people\" + 0.001*\"get\" + 0.001*\"great\" + 0.001*\"please\" + 0.001*\"one\" + 0.001*\"home\" + 0.001*\"stay\" + 0.001*\"new\" + 0.001*\"help\" + 0.001*\"like\" + 0.001*\"happy\" + 0.001*\"morning\" + 0.001*\"need\" + 0.001*\"going\" + 0.001*\"work\" + 0.001*\"best\" + 0.001*\"still\"\n",
      "\n",
      "Topic: 1 \n",
      " Word: 0.001*\"new\" + 0.001*\"one\" + 0.001*\"us\" + 0.001*\"not\" + 0.001*\"day\" + 0.001*\"time\" + 0.001*\"today\" + 0.001*\"home\" + 0.001*\"check\" + 0.001*\"camp\" + 0.001*\"like\" + 0.001*\"covid\" + 0.001*\"love\" + 0.001*\"get\" + 0.001*\"good\" + 0.001*\"coronavirus\" + 0.001*\"social\" + 0.001*\"live\" + 0.001*\"family\" + 0.001*\"video\" + 0.001*\"go\" + 0.001*\"via\" + 0.001*\"work\" + 0.001*\"see\" + 0.001*\"keep\"\n",
      "\n",
      "Topic: 2 \n",
      " Word: 0.001*\"week\" + 0.001*\"see\" + 0.001*\"latest\" + 0.001*\"twitter\" + 0.001*\"via\" + 0.001*\"thanks\" + 0.001*\"photo\" + 0.001*\"stayindoorsdreamoutdoors\" + 0.001*\"daily\" + 0.001*\"party_popper\" + 0.001*\"thank\" + 0.001*\"camp\" + 0.001*\"get\" + 0.001*\"home\" + 0.001*\"retweet\" + 0.001*\"time\" + 0.001*\"reach\" + 0.001*\"not\" + 0.001*\"posted\" + 0.001*\"today\" + 0.001*\"great\" + 0.001*\"k\" + 0.001*\"like\" + 0.001*\"likes\" + 0.001*\"day\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf_active20.print_topics(num_topics=-1,num_words=25):\n",
    "    print('Topic: {} \\n Word: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 Twitter topics in 2020 among active users:<br>\n",
    "1) Coronavirus staying at home and trying to stay positive (us, best, great)<br>\n",
    "2) Coronavirus keeping in touch via social media (family, video, social) <br>\n",
    "3) Missing outdoors during Coronavirus (#stayindoorsdreamoutdoors)<br>\n",
    "All the words were about the same term frequency, so the topic themes are not very evident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Topics for Lazy Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy 19\n",
    "lda_model_tfidf_lazy19 = gensim.models.LdaMulticore(corpus_tfidf_lazy19, num_topics=3, id2word=dict_lazy19, passes =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      " Word: 0.001*\"like\" + 0.001*\"one\" + 0.001*\"new\" + 0.001*\"love\" + 0.001*\"day\" + 0.001*\"get\" + 0.001*\"not\" + 0.001*\"today\" + 0.001*\"never\" + 0.001*\"lol\" + 0.001*\"even\" + 0.001*\"think\" + 0.001*\"got\" + 0.001*\"check\" + 0.001*\"live\" + 0.001*\"work\" + 0.001*\"little\" + 0.001*\"going\" + 0.001*\"anyone\" + 0.001*\"th\" + 0.001*\"us\" + 0.001*\"go\" + 0.001*\"know\" + 0.001*\"twitter\" + 0.001*\"time\"\n",
      "\n",
      "Topic: 1 \n",
      " Word: 0.002*\"new\" + 0.001*\"not\" + 0.001*\"going\" + 0.001*\"happy\" + 0.001*\"life\" + 0.001*\"week\" + 0.001*\"day\" + 0.001*\"get\" + 0.001*\"time\" + 0.001*\"like\" + 0.001*\"one\" + 0.001*\"free\" + 0.001*\"last\" + 0.001*\"please\" + 0.001*\"love\" + 0.001*\"bestbuyatplaylistlive\" + 0.001*\"bestbuy\" + 0.001*\"teamcanon\" + 0.001*\"ever\" + 0.001*\"people\" + 0.001*\"much\" + 0.001*\"hey\" + 0.001*\"best\" + 0.001*\"year\" + 0.001*\"know\"\n",
      "\n",
      "Topic: 2 \n",
      " Word: 0.002*\"college\" + 0.002*\"hate\" + 0.001*\"not\" + 0.001*\"love\" + 0.001*\"back\" + 0.001*\"one\" + 0.001*\"time\" + 0.001*\"newprofilepic\" + 0.001*\"see\" + 0.001*\"dont\" + 0.001*\"need\" + 0.001*\"day\" + 0.001*\"life\" + 0.001*\"like\" + 0.001*\"really\" + 0.001*\"days\" + 0.001*\"go\" + 0.001*\"come\" + 0.001*\"home\" + 0.001*\"first\" + 0.001*\"right\" + 0.001*\"oh\" + 0.001*\"good\" + 0.001*\"know\" + 0.001*\"sure\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf_lazy19.print_topics(num_topics=-1,num_words=25):\n",
    "    print('Topic: {} \\n Word: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2019 the top 3 topics for lazy users were:<br>\n",
    "1) General Twitter chatter  <br>\n",
    "2) Best Buy Playlist Live event <br>\n",
    "3) Not enjoying college <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy 20\n",
    "lda_model_tfidf_lazy20 = gensim.models.LdaMulticore(corpus_tfidf_lazy20, num_topics=3, id2word=dict_lazy20, passes =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      " Word: 0.001*\"like\" + 0.001*\"good\" + 0.001*\"need\" + 0.001*\"week\" + 0.001*\"twitter\" + 0.001*\"today\" + 0.001*\"see\" + 0.001*\"get\" + 0.001*\"not\" + 0.001*\"day\" + 0.001*\"covid\" + 0.001*\"stay\" + 0.001*\"right\" + 0.001*\"time\" + 0.001*\"work\" + 0.001*\"home\" + 0.001*\"love\" + 0.001*\"say\" + 0.001*\"feel\" + 0.001*\"people\" + 0.001*\"one\" + 0.001*\"social\" + 0.001*\"red_heart\" + 0.001*\"going\" + 0.001*\"party_popper\" + 0.001*\"thanks\" + 0.001*\"life\" + 0.001*\"go\" + 0.001*\"happy\" + 0.001*\"god\"\n",
      "\n",
      "Topic: 1 \n",
      " Word: 0.002*\"not\" + 0.002*\"want\" + 0.002*\"miss\" + 0.001*\"going\" + 0.001*\"like\" + 0.001*\"go\" + 0.001*\"really\" + 0.001*\"na\" + 0.001*\"quarantine\" + 0.001*\"day\" + 0.001*\"one\" + 0.001*\"wan\" + 0.001*\"get\" + 0.001*\"hair\" + 0.001*\"love\" + 0.001*\"people\" + 0.001*\"time\" + 0.001*\"got\" + 0.001*\"know\" + 0.001*\"face_with_tears_of_joy\" + 0.001*\"fuck\" + 0.001*\"home\" + 0.001*\"new\" + 0.001*\"today\" + 0.001*\"newprofilepic\" + 0.001*\"back\" + 0.001*\"good\" + 0.001*\"work\" + 0.001*\"look\" + 0.001*\"check\"\n",
      "\n",
      "Topic: 2 \n",
      " Word: 0.003*\"not\" + 0.002*\"time\" + 0.002*\"like\" + 0.002*\"day\" + 0.002*\"get\" + 0.002*\"people\" + 0.002*\"today\" + 0.002*\"going\" + 0.002*\"one\" + 0.002*\"know\" + 0.002*\"quarantine\" + 0.001*\"still\" + 0.001*\"love\" + 0.001*\"acnh\" + 0.001*\"much\" + 0.001*\"good\" + 0.001*\"animalcrossing\" + 0.001*\"never\" + 0.001*\"got\" + 0.001*\"home\" + 0.001*\"back\" + 0.001*\"need\" + 0.001*\"really\" + 0.001*\"would\" + 0.001*\"think\" + 0.001*\"go\" + 0.001*\"nintendoswitch\" + 0.001*\"even\" + 0.001*\"want\" + 0.001*\"make\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf_lazy20.print_topics(num_topics=-1,num_words=30):\n",
    "    print('Topic: {} \\n Word: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2020 the top 3 topics for lazy users were:<br>\n",
    "1) Covid support and activities (home, red_heart, party_pooper)<br>\n",
    "2) Quarantine emotions (face_with_tears_of_joy, miss) <br>\n",
    "3) Quarantine activities (playing Animal Crossing and Nintendo Switch) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Negative Matrix Factorization (NMF) in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike LDA, NMF relies on linear algebra. It approximates a nonnegative matrix by the product of two low-rank nonnegative matrices. Since it gives semantically meaningful result that is easily interpretable in clustering applications, NMF has been widely used as a clustering method especially for document data, and as a topic modeling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NMF\n",
    "active19_nmf = NMF(n_components=3).fit(active19_tfidf) #, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd'\n",
    "active20_nmf = NMF(n_components=3).fit(active20_tfidf)\n",
    "lazy19_nmf = NMF(n_components=3).fit(lazy19_tfidf)\n",
    "lazy20_nmf = NMF(n_components=3).fit(lazy20_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to display topics\n",
    "def display_topics(docs, model, no_top_words):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf = tfidf_vectorizer.fit_transform(docs)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Topics for Active Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "facebook posted video new httpstcovxvtekam httpstcoltxkwnxca httpstcoovmahqwpnb httpstcobtfmshya httpstcoktntegrafp httpstcovzwgxtrz httpstcomjcwhnr httpstcoboppqyuy httpstcokxmzfjlo httpstcopjrjhty httpstcocxufmhhli httpstcolqguotxcq httpstcobstkcql httpstcobcwbiqel httpstcosqaptxw httpstcoslwobsjq httpstcoqxzcihjfy httpstcojtdconsund httpstcosquanfmu httpstcoxcaiqoipl httpstcoqgkjcrlpra httpstcoqypphvvb httpstcokqcbdxiiyi httpstcokbjbztjyb httpstcoalmyutmq httpstcoaahgpow\n",
      "Topic 1:\n",
      "great day us one time check spring get today see love like go april good weekend season not new th hiking come best happy join fun first friends looking week\n",
      "Topic 2:\n",
      "camp sagar rajasthan chhatra india httpstcooxavwukax luxurycamps luxurycamp wilderness around walk nature tent spring httpstcooxavwuzyv campseriessentelevisioncom theshowdownsen theshowdown regional invited neelgai film dungari tents moti email chance httpstcoluuyjti httpstcohkxwyzb httpstcohpmruymf\n"
     ]
    }
   ],
   "source": [
    "# active 2019:\n",
    "display_topics(active19_docs, active19_nmf, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 3 topics for active users in 2019 were:<br>\n",
    "1) Links to Facebook videos<br>\n",
    "2) Great time hiking in the Spring<br>\n",
    "3) Advertising of camping in India by chhatrasagar Twitter user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "timeout usba dayathewoods getbetter homebivouac nothingbasically todaysnake onelebanonngo stayfit covidaustralia goodcall lildickytweets newcollection greatdaysout pepperelleddie helpeveryone safleoedd knowturning negative workgloves plesently goalie makesiteasier goingwell keephawaiicooking back_arrow lovefife thankyoupolis wellness coronavirusbill\n",
      "Topic 1:\n",
      "two_heartstwo_heartstwo_hearts readindie partying_faceparty_popper weeklymealplan seeeeee revenant lily mera revered mepolitics representation representationmatters fanswhich biggreenegg thankyoupolis vickibrightwood httpstcogrpoq followsclapping_handsclapping_handsclapping_hands httpstcoknrjdolfle httpstcotheotgzs httpstcoddtgwup httpstcozhsvxmwqq httpstcokjpcz httpstcolwcyzpsafo httpstcotftlkkkxx httpstcotztcwzz httpstcofvhqsp httpstcokcyroklfa httpstcoeqbzvpme httpstcotvtkihysgc\n",
      "Topic 2:\n",
      "latulipmike dailydoseofgreenspace thatchedroof newsreading coronavirusbill covidaustralia outdoorstyle hikinga spouses huntingusa wildnessat fishburn campingand outdoordesign natureistheway millersville digitalhug hawkwood susanorlean checker kidsshoes leguano salishsea trailrunmag teeing organiccannabis wooleryha upinvs grinning_cat_face_with_smiling_eyes activities\n"
     ]
    }
   ],
   "source": [
    "# active 2020:\n",
    "display_topics(active20_docs, active20_nmf, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 3 topics for active users in 2020 were:<br>\n",
    "1) Coronavirus (stay at home, wellness, #getbetter, etc.) <br>\n",
    "2) Thanking people (emojis of clapping_hands,two_hearts, #thankopolis, etc.)<br>\n",
    "3) Activities during Coronavirus (newsreading, hiking, camping, #dailydoseofgreenspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Topics for Active Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "college hate fucking really httpstcoeagccrgpvd beach reason song another curiouscat deactivate likely httpstcoevffrco ur youll ima game send due anymore reminded intro woman_facepalming_light_skin_tone truth thrones brain done enrolled face_with_rolling_eyesface_with_rolling_eyesface_with_rolling_eyes medication\n",
      "Topic 1:\n",
      "not one love new day like get going back time people want see today never life dont know us go need got much best year first happy feel great good\n",
      "Topic 2:\n",
      "bestbuyatplaylistlive bestbuy teamcanon win let daughter meetup httpstcorgylmtmvr httpstcocrctdgit giveaway enter entered pusheen tiger roll gift card httpstconiudjkvdxs fearful_face cash national grand giant pc hope httpstcoatphnri httpstcoptdjmnah boosttiger tsampcs shop\n"
     ]
    }
   ],
   "source": [
    "# lazy 2019:\n",
    "display_topics(lazy19_docs, lazy19_nmf, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 3 topics for lazy users in 2019 were:<br>\n",
    "1) Hating college (hate, college, face_with_rolling_eyes emoji, etc.) <br>\n",
    "2) Good day feeling great (best, day, happy, great, etc.)<br>\n",
    "3) Twitter contests (bestbuyatplaylistlive, gift card, meetup, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "nouns lincoln tinuod gettingbetter golfing periodpiece onte kokoresign reclaimingmyhealth godblessthedishwasher goodreads homeorwork watchedrewatched lowemissions nemesisabitch backe sektorunda wottitotsclub stops riskchronicles thought_balloon wt govermentface_with_symbols_on_mouthface_with_symbols_on_mouthface_with_symbols_on_mouth multiplexes felineboyfriend everybody malaysianpolitics doomed newyorklockdown stefanlöfven\n",
      "Topic 1:\n",
      "acnhdesign animalcrossingdodocode noch lowemissions issued cutoff animalfacts longsmiling_face_with_heart govermentface_with_symbols_on_mouthface_with_symbols_on_mouthface_with_symbols_on_mouth shockandawe httpstconisfz httpstcobymckdgsy httpstcoevprpgnn findings how wolves nextyear animalcrossingdesign harapkan volqx oliviareidxo overflow httpstcodxjgyxgwd enright multiplexes httpstcoucyvkxaa thedarkknightrises crowned yorme gamitin\n",
      "Topic 2:\n",
      "daylong queensono tomatoes evil_monkey harapkan goodreads maggid homeorwork motivating wottitotsclub onte bisag fishflan aprilmay daystodie wowzasmiling_face_with_heart how thebroadmuseum horoscopes govermentface_with_symbols_on_mouthface_with_symbols_on_mouthface_with_symbols_on_mouth mcdermotteng endog isyanı thoroughly ghiucari haley alres monicadolan houstonpress evil_monkeyrolling_on_the_floor_laughingrolling_on_the_floor_laughingrolling_on_the_floor_laughing\n"
     ]
    }
   ],
   "source": [
    "# lazy 2020:\n",
    "display_topics(lazy20_docs, lazy20_nmf, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 3 topics for lazy users in 2020 were:<br>\n",
    "1) Lockdown and staying at home (#homeorwork, #reclaimingmyhealth, #newyorklockdown, etc.) <br>\n",
    "2) Playing online games (#achndesign, animalcrossing, #gameitin, etc.)<br>\n",
    "3) Other activities (goodreads, #homeorwork, #daylong, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
