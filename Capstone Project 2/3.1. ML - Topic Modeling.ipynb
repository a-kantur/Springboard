{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "active20=pd.read_csv(\"active20-clean.csv\")\n",
    "active19=pd.read_csv(\"active19-clean.csv\")\n",
    "lazy20=pd.read_csv(\"lazy20-clean.csv\")\n",
    "lazy19=pd.read_csv(\"lazy19-clean.csv\")\n",
    "twitter_users = pd.read_csv(\"Twitter users.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this section we will try to answer the question: <br>\n",
    "What Did Active and Lazy User Cohorts Discuss on Twitter in Spring 2019 and in Spring 2020?<br>\n",
    "To answer this question, we will use topic modeling. Topic modeling is an unsupervised machine learning technique. This means it can infer patterns and cluster similar expressions without needing to define topic tags or train data beforehand. We will use this approach to identify main topics of dicsussion in 2019 and 2020 in both cohorts. Based on the EDA findings, we expect to see weather-related topic in active 2019 tweets, game-related topics in lazy 2019 tweets and covid-related topic in both active and lazy in 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling algorithms are statistical methods that analyze the words of documents to discover the themes that pervade a large collection of documents. The basic idea of topic modeling is that a document is a mixture of latent topics and each topic is expressed by a distribution of words. Latent Dirichlet Allocation (LDA) is the most popular topic modeling method in the field of text mining. The output of LDA provides two probability matrices: 1) the (posterior) probability distribution of each document over the topics, and 2) the probability distribution of words in a given topic. The calculated probability matrixes are used to make inference about the topics and documents for text mining. LDA has been shown to be an effective tool for text mining of large datasets. We will then do topic model-derived clustering based on highest probable topic assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating list of documents from the preprocessed tweets:\n",
    "active19_docs=active19.clean_text.dropna()\n",
    "active19_docs=active19_docs.to_list()\n",
    "active20_docs=active20.clean_text.dropna()\n",
    "active20_docs=active20_docs.to_list()\n",
    "lazy19_docs=lazy19.clean_text.dropna()\n",
    "lazy19_docs=lazy19_docs.to_list()\n",
    "lazy20_docs=lazy20.clean_text.dropna()\n",
    "lazy20_docs=lazy20_docs.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# tokenize the tweet docs (we lemmatize the words in tweet docs \n",
    "# in order to reduce inflectional forms to a common base form):\n",
    "active19_docs_tokenized = [word_tokenize(doc) for doc in active19_docs]\n",
    "active20_docs_tokenized = [word_tokenize(doc) for doc in active20_docs]\n",
    "lazy19_docs_tokenized = [word_tokenize(doc) for doc in lazy19_docs]\n",
    "lazy20_docs_tokenized = [word_tokenize(doc) for doc in lazy20_docs]\n",
    "\n",
    "# make a gensim dictionary with tokenized tweet docs:\n",
    "dict_active19 = Dictionary(active19_docs_tokenized)\n",
    "dict_active20 = Dictionary(active20_docs_tokenized)\n",
    "dict_lazy19 = Dictionary(lazy19_docs_tokenized)\n",
    "dict_lazy20 = Dictionary(lazy20_docs_tokenized)\n",
    "\n",
    "# create a gensim corpus (different from the regular corpus in so that \n",
    "# each document is converted to a bag of words using token ids):\n",
    "active19_corpus = [dict_active19.doc2bow(doc) for doc in active19_docs_tokenized] \n",
    "active20_corpus = [dict_active20.doc2bow(doc) for doc in active20_docs_tokenized] \n",
    "lazy19_corpus = [dict_lazy19.doc2bow(doc) for doc in lazy19_docs_tokenized] \n",
    "lazy20_corpus = [dict_lazy20.doc2bow(doc) for doc in lazy20_docs_tokenized] \n",
    "\n",
    "# how to go to the word behind a token\n",
    "# dict_active19.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new', 'knife', 'post', 'forum', 'new', 'opinel', 'httpstcoprbmwht', 'knives']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active19_docs_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD-IDF model to define word frequency:\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf_active19 = models.TfidfModel(active19_corpus)\n",
    "corpus_tfidf_active19 = tfidf_active19[active19_corpus]\n",
    "#from pprint import pprint\n",
    "#for doc in corpus_tfidf_active19:\n",
    "#    pprint(doc)\n",
    "#    break\n",
    "tfidf_active20 = models.TfidfModel(active20_corpus)\n",
    "corpus_tfidf_active20 = tfidf_active20[active20_corpus]\n",
    "\n",
    "tfidf_lazy19 = models.TfidfModel(lazy19_corpus)\n",
    "corpus_tfidf_lazy19 = tfidf_lazy19[lazy19_corpus]\n",
    "\n",
    "tfidf_lazy20 = models.TfidfModel(lazy20_corpus)\n",
    "corpus_tfidf_lazy20 = tfidf_lazy20[lazy20_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Topics for Active Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running LDA using TF-IDF\n",
    "\n",
    "# Active 19\n",
    "lda_model_tfidf_active19 = gensim.models.LdaMulticore(corpus_tfidf_active19, num_topics=3, id2word=dict_active19, passes =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      " Word: 0.001*\"new\" + 0.001*\"facebook\" + 0.001*\"check\" + 0.001*\"see\" + 0.001*\"video\" + 0.001*\"great\" + 0.001*\"not\" + 0.001*\"posted\" + 0.001*\"get\" + 0.001*\"love\" + 0.001*\"one\" + 0.001*\"us\" + 0.001*\"day\" + 0.001*\"week\" + 0.001*\"today\" + 0.001*\"come\" + 0.001*\"spring\" + 0.001*\"ready\" + 0.001*\"weekend\" + 0.001*\"join\" + 0.001*\"morning\" + 0.001*\"go\" + 0.001*\"like\" + 0.001*\"work\" + 0.001*\"find\"\n",
      "\n",
      "Topic: 1 \n",
      " Word: 0.001*\"spring\" + 0.001*\"great\" + 0.001*\"new\" + 0.001*\"one\" + 0.001*\"us\" + 0.001*\"get\" + 0.001*\"love\" + 0.001*\"time\" + 0.001*\"check\" + 0.001*\"happy\" + 0.001*\"day\" + 0.001*\"please\" + 0.001*\"fun\" + 0.001*\"see\" + 0.001*\"video\" + 0.001*\"last\" + 0.001*\"going\" + 0.001*\"like\" + 0.001*\"life\" + 0.001*\"live\" + 0.001*\"back\" + 0.001*\"good\" + 0.001*\"night\" + 0.001*\"today\" + 0.001*\"dont\"\n",
      "\n",
      "Topic: 2 \n",
      " Word: 0.001*\"day\" + 0.001*\"new\" + 0.001*\"great\" + 0.001*\"time\" + 0.001*\"hiking\" + 0.001*\"check\" + 0.001*\"today\" + 0.001*\"april\" + 0.001*\"good\" + 0.001*\"video\" + 0.001*\"one\" + 0.001*\"best\" + 0.001*\"season\" + 0.001*\"get\" + 0.001*\"us\" + 0.001*\"th\" + 0.001*\"like\" + 0.001*\"go\" + 0.001*\"via\" + 0.001*\"camp\" + 0.001*\"facebook\" + 0.001*\"life\" + 0.001*\"adventure\" + 0.001*\"look\" + 0.001*\"looking\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf_active19.print_topics(num_topics=-1,num_words=25):\n",
    "    print('Topic: {} \\n Word: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from LDA model with term frequency, in 2019 the active users Twitter topics can be split into 3 groups:<br>\n",
    "1) Happy time hiking <br>\n",
    "2) Social media (Facebook, videos) about the weekend fun outdoors  <br> \n",
    "3) Social media (Facebook, videos) about camping and the Spring season <br>\n",
    "However, term frequency was almost the same, and none of the words in any topic were particularly important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active 20\n",
    "lda_model_tfidf_active20 = gensim.models.LdaMulticore(corpus_tfidf_active20, num_topics=3, id2word=dict_active20, passes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      " Word: 0.002*\"covid\" + 0.001*\"day\" + 0.001*\"us\" + 0.001*\"time\" + 0.001*\"coronavirus\" + 0.001*\"good\" + 0.001*\"not\" + 0.001*\"today\" + 0.001*\"people\" + 0.001*\"get\" + 0.001*\"great\" + 0.001*\"please\" + 0.001*\"one\" + 0.001*\"home\" + 0.001*\"stay\" + 0.001*\"new\" + 0.001*\"help\" + 0.001*\"like\" + 0.001*\"happy\" + 0.001*\"morning\" + 0.001*\"need\" + 0.001*\"going\" + 0.001*\"work\" + 0.001*\"best\" + 0.001*\"still\"\n",
      "\n",
      "Topic: 1 \n",
      " Word: 0.001*\"new\" + 0.001*\"one\" + 0.001*\"us\" + 0.001*\"not\" + 0.001*\"day\" + 0.001*\"time\" + 0.001*\"today\" + 0.001*\"home\" + 0.001*\"check\" + 0.001*\"camp\" + 0.001*\"like\" + 0.001*\"covid\" + 0.001*\"love\" + 0.001*\"get\" + 0.001*\"good\" + 0.001*\"coronavirus\" + 0.001*\"social\" + 0.001*\"live\" + 0.001*\"family\" + 0.001*\"video\" + 0.001*\"go\" + 0.001*\"via\" + 0.001*\"work\" + 0.001*\"see\" + 0.001*\"keep\"\n",
      "\n",
      "Topic: 2 \n",
      " Word: 0.001*\"week\" + 0.001*\"see\" + 0.001*\"latest\" + 0.001*\"twitter\" + 0.001*\"via\" + 0.001*\"thanks\" + 0.001*\"photo\" + 0.001*\"stayindoorsdreamoutdoors\" + 0.001*\"daily\" + 0.001*\"party_popper\" + 0.001*\"thank\" + 0.001*\"camp\" + 0.001*\"get\" + 0.001*\"home\" + 0.001*\"retweet\" + 0.001*\"time\" + 0.001*\"reach\" + 0.001*\"not\" + 0.001*\"posted\" + 0.001*\"today\" + 0.001*\"great\" + 0.001*\"k\" + 0.001*\"like\" + 0.001*\"likes\" + 0.001*\"day\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf_active20.print_topics(num_topics=-1,num_words=25):\n",
    "    print('Topic: {} \\n Word: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3 Twitter topics in 2020 among active users:<br>\n",
    "1) Coronavirus staying at home and trying to stay positive (us, best, great)<br>\n",
    "2) Coronavirus keeping in touch via social media (family, video, social) <br>\n",
    "3) Missing outdoors during Coronavirus (#stayindoorsdreamoutdoors)<br>\n",
    "All the words were about the same term frequency, so the topic themes are not very evident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Topics for Lazy Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy 19\n",
    "lda_model_tfidf_lazy19 = gensim.models.LdaMulticore(corpus_tfidf_lazy19, num_topics=3, id2word=dict_lazy19, passes =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      " Word: 0.001*\"like\" + 0.001*\"one\" + 0.001*\"new\" + 0.001*\"love\" + 0.001*\"day\" + 0.001*\"get\" + 0.001*\"not\" + 0.001*\"today\" + 0.001*\"never\" + 0.001*\"lol\" + 0.001*\"even\" + 0.001*\"think\" + 0.001*\"got\" + 0.001*\"check\" + 0.001*\"live\" + 0.001*\"work\" + 0.001*\"little\" + 0.001*\"going\" + 0.001*\"anyone\" + 0.001*\"th\" + 0.001*\"us\" + 0.001*\"go\" + 0.001*\"know\" + 0.001*\"twitter\" + 0.001*\"time\"\n",
      "\n",
      "Topic: 1 \n",
      " Word: 0.002*\"new\" + 0.001*\"not\" + 0.001*\"going\" + 0.001*\"happy\" + 0.001*\"life\" + 0.001*\"week\" + 0.001*\"day\" + 0.001*\"get\" + 0.001*\"time\" + 0.001*\"like\" + 0.001*\"one\" + 0.001*\"free\" + 0.001*\"last\" + 0.001*\"please\" + 0.001*\"love\" + 0.001*\"bestbuyatplaylistlive\" + 0.001*\"bestbuy\" + 0.001*\"teamcanon\" + 0.001*\"ever\" + 0.001*\"people\" + 0.001*\"much\" + 0.001*\"hey\" + 0.001*\"best\" + 0.001*\"year\" + 0.001*\"know\"\n",
      "\n",
      "Topic: 2 \n",
      " Word: 0.002*\"college\" + 0.002*\"hate\" + 0.001*\"not\" + 0.001*\"love\" + 0.001*\"back\" + 0.001*\"one\" + 0.001*\"time\" + 0.001*\"newprofilepic\" + 0.001*\"see\" + 0.001*\"dont\" + 0.001*\"need\" + 0.001*\"day\" + 0.001*\"life\" + 0.001*\"like\" + 0.001*\"really\" + 0.001*\"days\" + 0.001*\"go\" + 0.001*\"come\" + 0.001*\"home\" + 0.001*\"first\" + 0.001*\"right\" + 0.001*\"oh\" + 0.001*\"good\" + 0.001*\"know\" + 0.001*\"sure\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf_lazy19.print_topics(num_topics=-1,num_words=25):\n",
    "    print('Topic: {} \\n Word: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2019 the top 3 topics for lazy users were:<br>\n",
    "1) General Twitter chatter  <br>\n",
    "2) Best Buy Playlist Live event <br>\n",
    "3) Not enjoying college <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy 20\n",
    "lda_model_tfidf_lazy20 = gensim.models.LdaMulticore(corpus_tfidf_lazy20, num_topics=3, id2word=dict_lazy20, passes =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      " Word: 0.001*\"like\" + 0.001*\"good\" + 0.001*\"need\" + 0.001*\"week\" + 0.001*\"twitter\" + 0.001*\"today\" + 0.001*\"see\" + 0.001*\"get\" + 0.001*\"not\" + 0.001*\"day\" + 0.001*\"covid\" + 0.001*\"stay\" + 0.001*\"right\" + 0.001*\"time\" + 0.001*\"work\" + 0.001*\"home\" + 0.001*\"love\" + 0.001*\"say\" + 0.001*\"feel\" + 0.001*\"people\" + 0.001*\"one\" + 0.001*\"social\" + 0.001*\"red_heart\" + 0.001*\"going\" + 0.001*\"party_popper\" + 0.001*\"thanks\" + 0.001*\"life\" + 0.001*\"go\" + 0.001*\"happy\" + 0.001*\"god\"\n",
      "\n",
      "Topic: 1 \n",
      " Word: 0.002*\"not\" + 0.002*\"want\" + 0.002*\"miss\" + 0.001*\"going\" + 0.001*\"like\" + 0.001*\"go\" + 0.001*\"really\" + 0.001*\"na\" + 0.001*\"quarantine\" + 0.001*\"day\" + 0.001*\"one\" + 0.001*\"wan\" + 0.001*\"get\" + 0.001*\"hair\" + 0.001*\"love\" + 0.001*\"people\" + 0.001*\"time\" + 0.001*\"got\" + 0.001*\"know\" + 0.001*\"face_with_tears_of_joy\" + 0.001*\"fuck\" + 0.001*\"home\" + 0.001*\"new\" + 0.001*\"today\" + 0.001*\"newprofilepic\" + 0.001*\"back\" + 0.001*\"good\" + 0.001*\"work\" + 0.001*\"look\" + 0.001*\"check\"\n",
      "\n",
      "Topic: 2 \n",
      " Word: 0.003*\"not\" + 0.002*\"time\" + 0.002*\"like\" + 0.002*\"day\" + 0.002*\"get\" + 0.002*\"people\" + 0.002*\"today\" + 0.002*\"going\" + 0.002*\"one\" + 0.002*\"know\" + 0.002*\"quarantine\" + 0.001*\"still\" + 0.001*\"love\" + 0.001*\"acnh\" + 0.001*\"much\" + 0.001*\"good\" + 0.001*\"animalcrossing\" + 0.001*\"never\" + 0.001*\"got\" + 0.001*\"home\" + 0.001*\"back\" + 0.001*\"need\" + 0.001*\"really\" + 0.001*\"would\" + 0.001*\"think\" + 0.001*\"go\" + 0.001*\"nintendoswitch\" + 0.001*\"even\" + 0.001*\"want\" + 0.001*\"make\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf_lazy20.print_topics(num_topics=-1,num_words=30):\n",
    "    print('Topic: {} \\n Word: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2020 the top 3 topics for lazy users were:<br>\n",
    "1) Covid support and activities (home, red_heart, party_pooper)<br>\n",
    "2) Quarantine emotions (face_with_tears_of_joy, miss) <br>\n",
    "3) Quarantine activities (playing Animal Crossing and Nintendo Switch) <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
